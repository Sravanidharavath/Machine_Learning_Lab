{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6aaec68-7287-4233-88f7-072f6a8f96ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 2, 'programming': 5, 'is': 1, 'intresting': 0, 'pizza': 4, 'very': 6, 'much': 3}\n",
      "[[0 0 1 0 0 1 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [0 0 1 1 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "d1=\"I love programming\"\n",
    "d2=\"programming is intresting\"\n",
    "d3=\"I love pizza very very much\"\n",
    "documents=[d1,d2,d3]\n",
    "vectorizer=CountVectorizer()\n",
    "X=vectorizer.fit_transform(documents)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc151d68-d789-486f-aac1-68dad2c5b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target       category\n",
      "0   From: mccall@mksol.dseg.ti.com (fred j mccall ...       2      sci.space\n",
      "1   From: \"Changyaw Wang\" <wangc@cs.indiana.edu>\\n...       1  comp.graphics\n",
      "2   From: lioness@maple.circa.ufl.edu\\nSubject: Te...       1  comp.graphics\n",
      "3   From: hotopp@ami1.bwi.wec.com (Daniel T. Hotop...       1  comp.graphics\n",
      "4   From: Ad-Robot@bobsbox.rent.com (Robotic Posti...       1  comp.graphics\n",
      "5   From: lee@hobbes.cs.umass.edu (Peter Lee)\\nSub...       1  comp.graphics\n",
      "6   From: gck@aero.org (Gregory C. Kozlowski)\\nSub...       0    alt.atheism\n",
      "7   From: PHARABOD@FRCPN11.IN2P3.FR\\nSubject: Fran...       2      sci.space\n",
      "8   From: wingo%cspara.decnet@Fedex.Msfc.Nasa.Gov\\...       2      sci.space\n",
      "9   From: nsmca@aurora.alaska.edu\\nSubject: Combo ...       2      sci.space\n",
      "10  From: gcdcrgm@state.systems.sa.gov.au\\nSubject...       1  comp.graphics\n",
      "11  From: aaron@minster.york.ac.uk\\nSubject: Re: D...       0    alt.atheism\n",
      "12  From: bobc@sed.stel.com (Bob Combs)\\nSubject: ...       2      sci.space\n",
      "13  From: rostroff@watson.princeton.edu (robert os...       2      sci.space\n",
      "14  From: af774@cleveland.Freenet.Edu (Chad Cipiti...       1  comp.graphics\n",
      "15  From: thomsonal@cpva.saic.com\\nSubject: Drag-f...       2      sci.space\n",
      "16  From: mathew <mathew@mantis.co.uk>\\nSubject: R...       0    alt.atheism\n",
      "17  From: aaronh@mksol.dseg.ti.com (Aaron Hightowe...       1  comp.graphics\n",
      "18  From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...       0    alt.atheism\n",
      "19  From: jan@vesta.neuroinformatik.ruhr-uni-bochu...       2      sci.space\n"
     ]
    }
   ],
   "source": [
    "#print 20% data\n",
    "import pandas as pd\n",
    "df=pd.read_csv('newsgroups_test.csv')\n",
    "print(df.iloc[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ad7c03-8bb7-4ff0-914c-248bb842018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1102 entries, 0 to 1101\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      1102 non-null   object\n",
      " 1   target    1102 non-null   int64 \n",
      " 2   category  1102 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 26.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f=pd.read_csv('newsgroups_test.csv')\n",
    "f.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c432a90-7e3f-4bf3-b7d0-c3681ce99fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  target       category\n",
      "0   From: mccall@mksol.dseg.ti.com (fred j mccall ...       2      sci.space\n",
      "1   From: \"Changyaw Wang\" <wangc@cs.indiana.edu>\\n...       1  comp.graphics\n",
      "2   From: lioness@maple.circa.ufl.edu\\nSubject: Te...       1  comp.graphics\n",
      "3   From: hotopp@ami1.bwi.wec.com (Daniel T. Hotop...       1  comp.graphics\n",
      "4   From: Ad-Robot@bobsbox.rent.com (Robotic Posti...       1  comp.graphics\n",
      "5   From: lee@hobbes.cs.umass.edu (Peter Lee)\\nSub...       1  comp.graphics\n",
      "6   From: gck@aero.org (Gregory C. Kozlowski)\\nSub...       0    alt.atheism\n",
      "7   From: PHARABOD@FRCPN11.IN2P3.FR\\nSubject: Fran...       2      sci.space\n",
      "8   From: wingo%cspara.decnet@Fedex.Msfc.Nasa.Gov\\...       2      sci.space\n",
      "9   From: nsmca@aurora.alaska.edu\\nSubject: Combo ...       2      sci.space\n",
      "10  From: gcdcrgm@state.systems.sa.gov.au\\nSubject...       1  comp.graphics\n",
      "11  From: aaron@minster.york.ac.uk\\nSubject: Re: D...       0    alt.atheism\n",
      "12  From: bobc@sed.stel.com (Bob Combs)\\nSubject: ...       2      sci.space\n",
      "13  From: rostroff@watson.princeton.edu (robert os...       2      sci.space\n",
      "14  From: af774@cleveland.Freenet.Edu (Chad Cipiti...       1  comp.graphics\n",
      "15  From: thomsonal@cpva.saic.com\\nSubject: Drag-f...       2      sci.space\n",
      "16  From: mathew <mathew@mantis.co.uk>\\nSubject: R...       0    alt.atheism\n",
      "17  From: aaronh@mksol.dseg.ti.com (Aaron Hightowe...       1  comp.graphics\n",
      "18  From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...       0    alt.atheism\n",
      "19  From: jan@vesta.neuroinformatik.ruhr-uni-bochu...       2      sci.space\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1102 entries, 0 to 1101\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      1102 non-null   object\n",
      " 1   target    1102 non-null   int64 \n",
      " 2   category  1102 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 26.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#load 20 \n",
    "df=pd.read_csv('newsgroups_test.csv')\n",
    "print(df.iloc[0:20])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2e692c-a0d0-4dc8-a336-e35d8560cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0]\n"
     ]
    }
   ],
   "source": [
    "#print all target labels\n",
    "import pandas as pd\n",
    "print(df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160cd8dc-bd92-404a-a716-abd568007b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ml/notebookenv/lib/python3.12/site-packages/sklearn/datasets/_base.py:1519: UserWarning: Retry downloading from url: https://ndownloader.figshare.com/files/5975967\n",
      "  warnings.warn(f\"Retry downloading from url: {remote.url}\")\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -3] Temporary failure in name resolution>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1336\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1382\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1381\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1470\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mConnect to a host on a given (SSL) port.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1001\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1000\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:828\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    827\u001b[39m exceptions = []\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    829\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:963\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    962\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    964\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mURLError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m categories = [\u001b[33m'\u001b[39m\u001b[33malt.atheism\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcomp.graphics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msci.space\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load subset of the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m newsgroups_subset = \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mheaders\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfooters\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquotes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Print number of documents per category\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal documents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(newsgroups_subset.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/sklearn/datasets/_twenty_newsgroups.py:320\u001b[39m, in \u001b[36mfetch_20newsgroups\u001b[39m\u001b[34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y, n_retries, delay)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[32m    319\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     cache = \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m20Newsgroups dataset not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/sklearn/datasets/_twenty_newsgroups.py:79\u001b[39m, in \u001b[36m_download_20newsgroups\u001b[39m\u001b[34m(target_dir, cache_path, n_retries, delay)\u001b[39m\n\u001b[32m     76\u001b[39m os.makedirs(target_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     78\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mDownloading dataset from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m (14 MB)\u001b[39m\u001b[33m\"\u001b[39m, ARCHIVE.url)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m archive_path = \u001b[43m_fetch_remote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mARCHIVE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mDecompressing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, archive_path)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tarfile.open(archive_path, \u001b[33m\"\u001b[39m\u001b[33mr:gz\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/sklearn/datasets/_base.py:1513\u001b[39m, in \u001b[36m_fetch_remote\u001b[39m\u001b[34m(remote, dirname, n_retries, delay)\u001b[39m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1512\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1513\u001b[39m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1514\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1515\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (URLError, \u001b[38;5;167;01mTimeoutError\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:240\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[32m    225\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m \u001b[33;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m url_type, path = _splittype(url)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.closing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    241\u001b[39m     headers = fp.info()\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m     req = meth(req)\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    518\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1347\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1344\u001b[39m         h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[32m   1345\u001b[39m                   encode_chunked=req.has_header(\u001b[33m'\u001b[39m\u001b[33mTransfer-encoding\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m   1348\u001b[39m     r = h.getresponse()\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[31mURLError\u001b[39m: <urlopen error [Errno -3] Temporary failure in name resolution>"
     ]
    }
   ],
   "source": [
    "#6.prepare subset of categories alt.athhesum com.graphics sci.space\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define categories to extract\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# Load subset of the dataset\n",
    "newsgroups_subset = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Print number of documents per category\n",
    "print(f\"Total documents: {len(newsgroups_subset.data)}\")\n",
    "print(f\"Target labels: {set(newsgroups_subset.target)}\")  # Numeric labels\n",
    "print(f\"Target names: {newsgroups_subset.target_names}\")  # Category names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0dd8e03-1822-4e84-9edf-baa16c8bbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 1657\n",
      "Target labels: {np.int64(0), np.int64(1), np.int64(2)}\n",
      "Target names: ['alt.atheism', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "#7.load 20 news group train subset with three categroies lt.athesim ; comp.graphic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define categories to extract\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.space']  # Modify if needed\n",
    "\n",
    "# Load the dataset (training subset)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"Total documents: {len(newsgroups_train.data)}\")\n",
    "print(f\"Target labels: {set(newsgroups_train.target)}\")  # Numeric labels\n",
    "print(f\"Target names: {newsgroups_train.target_names}\")  # Category names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8122a856-7c79-43b7-bc2e-1ec1ea677ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 1102\n",
      "Target labels: {np.int64(0), np.int64(1), np.int64(2)}\n",
      "Target names: ['alt.atheism', 'comp.graphics', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "#8.load 20 news group test subset categories alt.athesim comp.graphics\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define categories to extract\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# Load the dataset (test subset)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"Total documents: {len(newsgroups_test.data)}\")\n",
    "print(f\"Target labels: {set(newsgroups_test.target)}\")  # Numeric labels\n",
    "print(f\"Target names: {newsgroups_test.target_names}\")  # Category names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e333e63e-b0cb-4779-a0c7-a20e3b52f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Names (Labels):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'newsgroups_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#9.print new training set target names(labels)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Print target names (labels)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTarget Names (Labels):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnewsgroups_test\u001b[49m.target_names)\n",
      "\u001b[31mNameError\u001b[39m: name 'newsgroups_test' is not defined"
     ]
    }
   ],
   "source": [
    "#9.print new training set target names(labels)\n",
    "# Print target names (labels)\n",
    "print(\"Target Names (Labels):\")\n",
    "print(newsgroups_test.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c99cae-0b02-461a-9a15-b55598d23cff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newsgroups_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#10.print new training data of 5th article\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnewsgroups_train\u001b[49m.data[\u001b[32m4\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'newsgroups_train' is not defined"
     ]
    }
   ],
   "source": [
    "#10.print new training data of 5th article\n",
    "print(newsgroups_train.data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3281ebf-3cac-4a49-9f28-6bdcf2612693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents (data shape): 1657\n",
      "Number of target labels: 1657\n"
     ]
    }
   ],
   "source": [
    "#11.print shape of data and targets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Define categories to extract\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "\n",
    "# Load the dataset (training subset)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Print the shape of data and targets\n",
    "print(\"Number of documents (data shape):\", len(newsgroups_train.data))\n",
    "print(\"Number of target labels:\", len(newsgroups_train.target))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee2dac79-8851-4617-a339-c2a817987bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.print training set filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13a807e-b532-4be2-a4cf-f84c2f856090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'from': 186, 'mccall': 269, 'mksol': 283, 'dseg': 153, 'ti': 426, 'com': 106, 'fred': 184, '575': 20, '3539': 12, 'subject': 402, 're': 348, 'satellite': 366, 'around': 59, 'pluto': 331, 'mission': 282, 'organization': 314, 'texas': 410, 'instruments': 223, 'inc': 216, 'lines': 246, '24': 7, 'in': 215, '1993apr30': 6, '004311': 0, 'aurora': 66, 'alaska': 37, 'edu': 158, 'nsmca': 303, 'writes': 484, 'being': 78, 'wierd': 475, 'again': 36, 'so': 387, 'be': 74, 'warned': 462, 'is': 228, 'there': 419, 'plan': 327, 'to': 428, 'put': 343, 'each': 154, 'planet': 328, 'the': 415, 'solar': 389, 'system': 405, 'keep': 233, 'watch': 464, 'help': 200, 'it': 229, 'better': 80, 'ask': 63, 'questions': 344, 'before': 77, 'spout': 395, 'an': 46, 'opinion': 311, 'how': 207, 'about': 27, 'unmanned': 443, 'stay': 397, 'orbit': 313, 'and': 47, 'record': 352, 'things': 422, 'near': 292, 'on': 307, 'know': 236, 'strange': 401, 'idea': 211, 'but': 90, 'why': 474, 'not': 301, 'could': 123, 'do': 141, 'some': 390, 'scanning': 367, 'of': 306, 'only': 310, 'also': 41, 'objects': 304, 'aaroundpluto': 24, 'as': 62, 'well': 468, 'seti': 379, 'looking': 255, 'at': 64, 'galaxy': 187, 'without': 479, 'having': 198, 'much': 289, 'worry': 482, 'doing': 143, 'this': 424, 'anything': 55, 'like': 244, 'reasonable': 351, 'time': 427, 'would': 483, 'require': 356, 'more': 286, 'propulsion': 340, 'capability': 97, 'than': 412, 'we': 465, 'can': 94, 'manage': 260, 'you': 485, 'have': 197, 'boost': 87, 'then': 418, 'slow': 385, 'back': 69, 'down': 147, 'something': 391, 'hohman': 201, 'think': 423, 'that': 414, 'take': 407, 'ridiculous': 358, 'amounts': 45, 'my': 290, 'rubber': 363, 'bible': 82, 'home': 202, 'insisting': 221, 'perfect': 325, 'safety': 365, 'for': 180, 'people': 323, 'who': 473, 'don': 145, 'balls': 70, 'live': 253, 'real': 350, 'world': 481, 'mary': 266, 'shafer': 380, 'nasa': 291, 'ames': 42, 'dryden': 152, 'speak': 393, 'others': 316, 'they': 421, 'me': 271, 'changyaw': 100, 'wang': 458, 'wangc': 459, 'cs': 127, 'indiana': 218, '3d': 15, 'graphics': 191, 'software': 388, 'company': 110, 'info': 219, 'computer': 112, 'science': 368, 'university': 442, 'believe': 79, 'many': 262, 'will': 476, 'happy': 194, 'information': 220, 'please': 329, 'post': 333, 'comp': 109, 'thanks': 413, 'lioness': 249, 'maple': 264, 'circa': 102, 'ufl': 439, 'tex': 409, 'texture': 411, 'map': 263, 'format': 181, 'center': 99, 'instructional': 222, 'research': 357, 'computing': 114, 'activities': 30, 'reply': 355, 'ufcc': 438, 'nntp': 300, 'posting': 334, 'host': 203, 'was': 463, 'avalon': 67, 'today': 429, 'found': 183, 'maps': 265, 'txc': 437, 've': 453, 'never': 296, 'encountered': 162, 'these': 420, 'are': 57, 'obviously': 305, 'or': 312, 'latex': 239, 'files': 179, 'if': 212, 'clue': 104, 'convert': 121, 'let': 241, 'brian': 88, 'hotopp': 205, 'ami1': 43, 'bwi': 91, 'wec': 466, 'daniel': 131, 'drivers': 151, 'diamond': 138, 'viper': 456, 'card': 98, 'westinghouse': 469, 'electronic': 160, 'systems': 406, 'group': 192, 'baltimore': 71, 'md': 270, '13': 3, 'been': 76, 'away': 68, 'couple': 124, 'weeks': 467, 'become': 75, 'out': 318, 'touch': 431, 'with': 478, 'latest': 238, 'does': 142, 'anyone': 54, 'has': 196, 'come': 108, 'any': 53, 'vesa': 455, 'driver': 150, 'updates': 445, 'lately': 237, 'wondering': 480, 'what': 470, 'windows': 477, 'version': 454, 'up': 444, 'now': 302, 'advance': 35, 'dan': 130, 'internet': 227, 'antenna': 52, 'microwave': 280, 'integration': 224, 'vax': 452, 'tron': 436, 'electric': 159, 'corp': 122, 'voice': 457, '410': 17, '765': 21, '2931': 8, 'ad': 31, 'robot': 360, 'bobsbox': 86, 'rent': 354, 'robotic': 361, 'menace': 275, 'your': 486, 'one': 308, 'stop': 399, 'shop': 382, 'root': 362, '102': 2, 'sharp': 381, 'stick': 398, 'eye': 171, 'andy': 48, 'meyer': 279, 'pleased': 330, 'announce': 50, 'just': 232, 'upgraded': 446, 'our': 317, 'new': 297, 'hayes': 199, 'ultra': 440, '144': 4, '32bis': 11, '42bis': 18, 'modem': 284, 'allow': 40, 'connections': 117, '300bps': 10, '38': 13, '400bps': 16, 'featuring': 176, 'specific': 394, 'those': 425, 'interested': 225, 'such': 403, 'image': 213, 'processing': 335, 'animation': 49, 'clip': 103, 'art': 61, 'public': 342, 'domain': 144, 'programs': 337, 'mailing': 258, 'lists': 252, 'imagine': 214, 'dctv': 133, 'lightwave': 243, 'aliased': 38, 'easy': 156, 'read': 349, 'forums': 182, 'which': 472, 'join': 231, 'free': 185, 'usenet': 449, 'access': 28, 'netmail': 293, 'sites': 384, 'all': 39, 'over': 319, 'ability': 25, 'contact': 118, 'hardware': 195, 'developers': 137, 'right': 359, 'their': 416, 'mainframes': 259, 'by': 92, 'sending': 375, 'them': 417, 'bbs': 73, 'mention': 276, 'kids': 235, 'college': 105, 'parents': 321, 'quick': 345, 'reliable': 353, 'links': 248, 'mail': 257, 'sent': 377, 'matter': 268, 'minutes': 281, 'unique': 441, 'conference': 115, 'bulletin': 89, 'section': 372, 'abilty': 26, 'track': 432, 'conferences': 116, 'enjoy': 163, 'own': 320, 'personal': 326, 'file': 178, 'area': 58, 'use': 448, 'messages': 278, 'proper': 339, 'create': 125, 'moderate': 285, 'newsfeeds': 299, 'include': 217, 'topics': 430, 'mac': 256, 'amiga': 44, 'ibm': 209, 'erotica': 166, 'ham': 193, 'radio': 346, 'star': 396, 'trek': 435, 'scuba': 370, 'diving': 140, 'programmers': 336, 'game': 188, 'designers': 136, 'cyberspace': 129, 'most': 287, 'sophisticated': 392, 'learn': 240, 'ever': 168, 'created': 126, 'pc': 322, 'easily': 155, 'delete': 135, 'move': 288, 'directory': 139, 'storage': 400, 'attach': 65, 'send': 374, 'another': 51, 'member': 274, 'smart': 386, 'sendmail': 376, 'feature': 174, 'ensures': 164, 'accurate': 29, 'addressing': 33, 'arpanet': 60, 'bitnet': 83, 'networks': 295, 'interface': 226, 'fidonet': 177, 'peoplenet': 324, 'uucp': 451, 'network': 294, 'facility': 173, 'library': 242, 'containing': 119, 'downloads': 148, 'popular': 332, 'computers': 113, 'features': 175, 'master': 267, 'listing': 250, 'newscan': 298, 'search': 371, 'capabilities': 96, 'complete': 111, 'archived': 56, 'listings': 251, 'supports': 404, 'transfer': 434, 'protocols': 341, 'hs': 208, 'link': 247, 'bi': 81, 'doubles': 146, 'server': 378, 'users': 450, 'other': 315, 'control': 120, 'environment': 165, 'select': 373, 'whether': 471, 'want': 460, 'hot': 204, 'key': 234, 'menus': 277, 'prompting': 338, 'etc': 167, 'define': 134, 'cancel': 95, 'choose': 101, 'terminal': 408, 'emulation': 161, 'edit': 157, 'login': 254, 'script': 369, 'online': 309, 'games': 189, 'global': 190, 'war': 461, 'jet': 230, 'combat': 107, 'simulator': 383, 'added': 32, 'manual': 261, 'extensive': 170, 'facilities': 172, 'tracking': 433, 'usage': 447, '170': 5, 'megabytes': 273, 'currently': 128, 'runs': 364, '8mhz': 22, 'meg': 272, 'ram': 347, 'ide': 210, 'drive': 149, 'call': 93, '908': 23, '469': 19, '0049': 1, '300': 9, '38400': 14, 'baud': 72, 'hours': 206, 'day': 132, 'everyday': 169, 'administator': 34, 'bob': 84, 'lindabury': 245, 'bobl': 85}\n",
      "Numerical representation:\n",
      " [[ 1  0  0 ...  1  2  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  1  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  1  1 ...  0 12 13]]\n"
     ]
    }
   ],
   "source": [
    "#13.by using Countvectorizer train data ito numerical format considering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "mf=(df.head())\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert text data into numerical form\n",
    "X = vectorizer.fit_transform(mf.text)\n",
    "\n",
    "# Display the vocabulary (words mapped to indices)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Display the transformed data as an array\n",
    "print(\"Numerical representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e085957f-d7cc-41d8-b19f-43cd564ea73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.use bernouli NM for training  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204da929-227e-495f-bc36-82f6c84dac5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897007a5-1036-4d61-a87c-9cce090dce1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eccf9092-8a03-4f7a-95e7-23738105e105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  predicted_label\n",
      "0     From: mccall@mksol.dseg.ti.com (fred j mccall ...                2\n",
      "1     From: \"Changyaw Wang\" <wangc@cs.indiana.edu>\\n...                1\n",
      "2     From: lioness@maple.circa.ufl.edu\\nSubject: Te...                1\n",
      "3     From: hotopp@ami1.bwi.wec.com (Daniel T. Hotop...                1\n",
      "4     From: Ad-Robot@bobsbox.rent.com (Robotic Posti...                1\n",
      "...                                                 ...              ...\n",
      "1097  From: malek@pi.titech.ac.jp (Zidouri Abdelmale...                1\n",
      "1098  From: livesey@solntze.wpd.sgi.com (Jon Livesey...                0\n",
      "1099  From: I3150101@dbstu1.rz.tu-bs.de (Benedikt Ro...                0\n",
      "1100  From: beck@irzr17.inf.tu-dresden.de (Andre Bec...                1\n",
      "1101  From: 18084TM@msu.edu (Tom)\\nSubject: Billboar...                2\n",
      "\n",
      "[1102 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#16.predict target labels for testing set\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the dataset (Assuming it's a CSV file with 'text' and 'label' columns)\n",
    "df = pd.read_csv(\"newsgroups_train.csv\")  # Replace with your actual dataset path\n",
    "test_df = pd.read_csv(\"newsgroups_test.csv\")  # Replace with your actual test dataset path\n",
    "\n",
    "# Step 2: Extract features and labels\n",
    "train_documents = df[\"text\"].astype(str)  # Convert text column to string\n",
    "train_labels = df[\"target\"]  # Labels (0, 1, 2, etc.)\n",
    "\n",
    "test_documents = test_df[\"text\"].astype(str)  # Convert test text column to string\n",
    "\n",
    "# Step 3: Convert text into numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(train_documents)\n",
    "X_test = vectorizer.transform(test_documents)  # Transform test data using the same vectorizer\n",
    "\n",
    "# Step 4: Train Bernoulli Naïve Bayes model\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, train_labels)\n",
    "\n",
    "# Step 5: Predict target values for test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Step 6: Save or print predictions\n",
    "test_df[\"predicted_label\"] = predictions  # Add predictions to test dataframe\n",
    "test_df.to_csv(\"predictions.csv\", index=False)  # Save predictions to CSV\n",
    "print(test_df[[\"text\", \"predicted_label\"]])  # Display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2573b261-2f51-4aef-80ed-b6efc2373d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8530\n"
     ]
    }
   ],
   "source": [
    "#17.find accuracy score on test set\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the dataset (Assuming it's a CSV file with 'text' and 'label' columns)\n",
    "df = pd.read_csv(\"newsgroups_train.csv\")  # Replace with your actual training dataset path\n",
    "test_df = pd.read_csv(\"newsgroups_test.csv\")  # Replace with your actual test dataset path\n",
    "\n",
    "# Step 2: Extract features and labels\n",
    "train_documents = df[\"text\"].astype(str)  # Convert text column to string\n",
    "train_labels = df[\"target\"]  # Labels (0, 1, 2, etc.)\n",
    "\n",
    "test_documents = test_df[\"text\"].astype(str)  # Convert test text column to string\n",
    "test_labels = test_df[\"target\"]  # True labels for accuracy calculation\n",
    "\n",
    "# Step 3: Convert text into numerical format using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_train = vectorizer.fit_transform(train_documents)\n",
    "X_test = vectorizer.transform(test_documents)  # Transform test data using the same vectorizer\n",
    "\n",
    "# Step 4: Train Bernoulli Naïve Bayes model\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, train_labels)\n",
    "\n",
    "# Step 5: Predict target values for test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Step 6: Compute Accuracy Score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")  # Print accuracy with 4 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbd2988-06db-4d8c-9c7d-939f50baf7a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'newsgroups_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Step 1: Load the dataset (Assuming it's a CSV file with 'text' and 'label' columns)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnewsgroups_train.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with your actual training dataset path\u001b[39;00m\n\u001b[32m      9\u001b[39m test_df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mnewsgroups_test.csv\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your actual test dataset path\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Step 2: Extract features and labels\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ml/notebookenv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'newsgroups_train.csv'"
     ]
    }
   ],
   "source": [
    "#18.use tfidvectorizer instead of count vectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the dataset (Assuming it's a CSV file with 'text' and 'label' columns)\n",
    "df = pd.read_csv(\"newsgroups_train.csv\")  # Replace with your actual training dataset path\n",
    "test_df = pd.read_csv(\"newsgroups_test.csv\")  # Replace with your actual test dataset path\n",
    "\n",
    "# Step 2: Extract features and labels\n",
    "train_documents = df[\"text\"].astype(str)  # Convert text column to string\n",
    "train_labels = df[\"target\"]  # Labels (0, 1, 2, etc.)\n",
    "\n",
    "test_documents = test_df[\"text\"].astype(str)  # Convert test text column to string\n",
    "test_labels = test_df[\"target\"]  # True labels for accuracy calculation\n",
    "\n",
    "# Step 3: Convert text into numerical format using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(binary=True)  # TF-IDF representation\n",
    "X_train = vectorizer.fit_transform(train_documents)\n",
    "X_test = vectorizer.transform(test_documents)  # Transform test data using the same vectorizer\n",
    "\n",
    "# Step 4: Train Bernoulli Naïve Bayes model\n",
    "model = BernoulliNB()\n",
    "model.fit(X_train, train_labels)\n",
    "\n",
    "# Step 5: Predict target values for test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Step 6: Compute Accuracy Score\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")  # Print accuracy with 4 decimal places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1caacf5-8114-447c-a257-e9e82afc1dc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#15. By using countvectorizer convert test data into numeric format considering only\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m f=\u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33mnewsgroups_test.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[32m      4\u001b[39m vectorizer = CountVectorizer()\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#15. By using countvectorizer convert test data into numeric format considering only\n",
    "f=pd.read_csv('newsgroups_test.csv')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Convert text data into numerical form\n",
    "X = vectorizer.fit_transform(df.text)\n",
    "\n",
    "# Display the vocabulary (words mapped to indices)\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Display the transformed data as an array\n",
    "md=print(\"Numerical representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c2b78-68b1-44e1-a0e3-211805c58705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99910da1-b3c5-47a8-ab1f-2416b82ab7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1ce16-b692-48c1-8f29-ab60704b4ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c91805-01c2-43d9-a7c3-395c17d55870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0670a-a9ed-4138-a576-76b5a7bfba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
